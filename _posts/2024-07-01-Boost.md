---
title: 경사하강법 
date: 2024-07-01 14:10:00 +0800
author: cotes
categories: [AI , Naver Boost]
tags: [Python, AI, 경사하강법]
render_with_liquid: false
---

# 2. 경사하강법 구현

## 1. 경사하강법
- 선형 회귀 모델을 구현하기 위한 데이터를 확인하고, 알맞은 형태로 분리합니다.
- 분리한 데이터를 활용해 train, weight, bias를 정의합니다.
- 경사하강법 구현을 위한 학습 루프를 만들고, 100회 반복해 결과를 출력합니다.

### [TODO] 1_ 데이터 분리


```python
import numpy as np

xy = np.array([[1., 2., 3., 4., 5., 6.],
               [10., 20., 30., 40., 50., 60.]])

## 코드시작 ##
x_train = xy[0]    
y_train = xy[1]   
## 코드종료 ##

print(x_train, x_train.shape)
print(y_train, y_train.shape)

```

    [1. 2. 3. 4. 5. 6.] (6,)
    [10. 20. 30. 40. 50. 60.] (6,)
    

### [TODO] 2_ train, weight, bias 정의


```python
## 코드시작 ##

np.random.seed(0)

beta_gd = np.random.rand(1) 
bias = np.random.rand(1)

## 코드종료 ##

print(beta_gd, bias)
```

    [0.5488135] [0.71518937]
    

### [TODO] 3_ 경사하강법 구현


```python
np.random.seed(0)
beta_gd = np.random.rand(1)
bias = np.random.rand(1)

# 다양한 학습률 설정
learning_rates = [0.001, 0.01, 0.1]

# 결과 저장
results = []

for lr in learning_rates:
    # 파라미터 초기화
    beta_gd = beta_gd.copy()
    bias = bias.copy()
    
    # 학습
    for i in range(1000):
        pred = beta_gd * x_train + bias
        error = ((pred - y_train) ** 2).mean()

        gd_w = ((pred - y_train) * 2 * x_train).mean()
        gd_b = ((pred - y_train) * 2).mean()

        beta_gd -= lr * gd_w
        bias -= lr * gd_b

        if i % 100 == 0:
            print(f'Learning Rate: {lr} | Epoch {i:4d}/1000 | Cost: {error:.6f} | w: {beta_gd.item():.6f} | b: {bias.item():.6f}')

    # 최종 비용 저장
    results.append((lr, error, beta_gd.item(), bias.item()))

# 결과 출력
for lr, final_error, final_beta, final_bias in results:
    print(f'Learning Rate: {lr} | Final Cost: {final_error:.6f} | Final w: {final_beta:.6f} | Final b: {final_bias:.6f}')

```

ALL RIGHTS RESERVED. (C)NAVER Connect Foundation.
